
*************************************************************************************
Preparation: Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1

Preparing training resources (datasets, vocabulary, dictionary, embedding, etc)...
Dataset: /home/luizmatos/Projetos/UFF/Python/reddit-topic-modelling/datasets/processed/reddit-posts-gatherer-submissions/reddit-posts-gatherer-en.submissions[processed].json
Dataset_name: Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1
Train_size: 0.8


Loading dataset file...
Dataset length: 32092
Loading Gensim dictionary...
No. of tokens on dictionary: 1524
Gensim vocabulary length: 1524
Loading word-lemma mappings...
Word-lemma and inverse mappings loaded with 2 entries
Filtering words not present in vocabulary...
Filtered non-vocabulary words
Documents length after filtering and removal of empty documents: 32072
Tokenizing documents and creating train dataset...
No. documents - Train: 25657	Test: 6415
Number of documents (train): 25657 [this should be equal to 25657]
Number of documents (test): 6415 [this should be equal to 6415]
Final number of documents (train): 25657
Final number of documents (test): 6415
Saving train documents file with 25657 documents (JSON): ./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/train_documents.json
Train documents file saved (JSON): ./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/train_documents.json
Saving joined test documents file with 6415 documents (JSON): ./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/test_documents.json
Test documents file saved (JSON): ./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/test_documents.json
Creating word dictionary for entire corpus...
Word dictionary created and saved to "./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/word_dictionary.gdict"
(ETM) Creating lists of words...
(ETM) Getting doc indices...
  len(np.unique(doc_indices_tr)): 25657 [this should be 25657]
  len(np.unique(doc_indices_ts)): 6415 [this should be 6415]
(ETM) Creating bow representation...
(ETM) Splitting bow intro token/value pairs and saving...
Creating ETM vocabulary file...
ETM vocabulary saved to "./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/etm_vocabulary.vocab"
Creating ETM training dataset file...
ETM training dataset saved to "./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/etm_training_dataset.dataset"
Creating ETM embeddings file with words in vocabulary
Generating optimized W2V embedding based on vocabulary words...


Generated optimized Gensim W2V embedding file at "./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/etm_w2v_embeddings.w2v"
1524/1524 words found on embeddings
ETM embeddings file with words in vocabulary created
Creating CTM training dataset file...
CTM: preprocessed_documents_for_bow = 25650
CTM: unpreprocessed_corpus_for_contextual = 25650
CTM: vocab = 1518
CTM data preparation instance saved to "./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/ctm_data_preparation.obj"
CTM training dataset saved to "./resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1/ctm_training_dataset.dataset"

Datasets prepared for training

Training resources prepared

Copying original preprocessed dataset to 'resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1' folder...

Moved dataset to 'resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1' folder

Copying dictionary to 'resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1' folder...

Moved dictionary to 'resources/Reddit_EN_NOUN_VERB_ADJ_tr0.8_mindf_0.005_maxdf_1' folder

*************************************************************************************
