Path:  /home/luizmatos/Projetos/UFF/Python/reddit-topic-modelling/datasets/original/brasil_desabafos_2008_2021/reddit-posts-gatherer-pt.submissions_[without_duplicates].json
Field:  body
Dataset:  reddit-posts-gatherer-pt.submissions
Folder:  reddit-posts-gatherer-submissions
Language:  pt
Is to lemmatize data?  True
Is to remove stopwords?  True
Is to remove POS categories?  True
POS categories to keep:  ['NOUN', 'VERB', 'ADJ']
Total of original documents: 3404
                                    _id  ...                                                url
0  {'$oid': '600876249f06905a08d662ef'}  ...  https://www.reddit.com/r/brasil/comments/hb425...
1  {'$oid': '600876259f06905a08d662f0'}  ...  https://www.reddit.com/r/desabafos/comments/hb...
2  {'$oid': '600876269f06905a08d662f1'}  ...  https://www.reddit.com/r/desabafos/comments/hb...
3  {'$oid': '600876269f06905a08d662f2'}  ...  https://www.reddit.com/r/desabafos/comments/hb...
4  {'$oid': '600876279f06905a08d662f3'}  ...  https://www.reddit.com/r/desabafos/comments/hb...

[5 rows x 26 columns]
Newlines and single-quotes removed from documents.
Tokenized documents.
Saved tokenized documents to temp file for further processing...
Lemmatized documents.
Word-lemma mapping created.
Lemma-word mapping created.
NOUN, VERB, ADJ POS categories of tokens kept and lemmatized.
Read processed documents from temp file
Using stoplist of length: 234
No. of stopwords removed: 15825
Stopwords removed.
Removed temporary files.
Size of data after preprocessing:  3404
Row count after removal of rows with empty "body" fields: 3400
Data dumped to  datasets/processed/reddit-posts-gatherer-submissions/reddit-posts-gatherer-pt.submissions[processed].json
Word-lemma and inverse mappings dumped to  datasets/processed/reddit-posts-gatherer-submissions/reddit-posts-gatherer-pt.submissions[word_lemma_maps].json
*******************



